{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bace15b",
   "metadata": {},
   "source": [
    "# Anagrams\n",
    "\n",
    "In this exercise we reiterate over the previous Anagrams example from TP1.\n",
    "We run a local Spark program to solve the exercise.\n",
    "\n",
    "Let's start by importing the Spark Context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a48de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379901d8",
   "metadata": {},
   "source": [
    "Next, we create an instance of SparkContext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95d587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3d407b",
   "metadata": {},
   "source": [
    "Now we want to get our data from HDFS.\n",
    "Let's check whether the file is already there:\n",
    "\n",
    "> Note that by starting the line with `!` we run a bash command instead of python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6cc687",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -ls common_words_en_subset.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05c7ebb",
   "metadata": {},
   "source": [
    "If the file don't show up you can put it on HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b749e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -put /vagrant/tp/1/common_words_en_subset.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7610e1",
   "metadata": {},
   "source": [
    "Next, we load the data into a Spark RDD (note that this command is lazily evaluated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d294c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sc.textFile('common_words_en_subset.txt')\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b359ea5",
   "metadata": {},
   "source": [
    "Let's display the content of the `words` RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48eb22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11854a9a",
   "metadata": {},
   "source": [
    "Observe the results; make sure the data has been successfully loaded.\n",
    "Then, run the map transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f2f125",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = words.map(lambda x: (''.join(sorted(list(x))), x))\n",
    "tuples.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b82b0a",
   "metadata": {},
   "source": [
    "Then, run the groupByKey (the rough equivalent to Hadoop’s « shuffle »):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69be811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = tuples.groupByKey().mapValues(lambda x: list(x))\n",
    "grouped.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb05a391",
   "metadata": {},
   "source": [
    "Observe the results. Then, run filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10380652",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = grouped.filter(lambda x: len(x[1])>1)\n",
    "filtered.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b334e613",
   "metadata": {},
   "source": [
    "Then, let's run mapValues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c73efc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = filtered.mapValues(lambda x: \", \".join(x))\n",
    "res1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53443642",
   "metadata": {},
   "source": [
    "Finally, also apply mapValues to the unfiltered RDD and save both datasets on\n",
    "disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5069596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = grouped.mapValues(lambda x: \", \".join(x))\n",
    "res2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bc4751",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1.saveAsTextFile('res-words-filtered')\n",
    "res2.saveAsTextFile('res-words-unfiltered')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc24123",
   "metadata": {},
   "source": [
    "Let's check that the results were written on HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f058008",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -ls res-words-filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e2f460",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -ls res-words-unfiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e24bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -cat res-words-filtered/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e324bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -cat res-words-unfiltered/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcebe2a4",
   "metadata": {},
   "source": [
    "Question: Why is the `grouped` RDD executed twice? How to force Spark to evaluate the `grouped` RDD only once?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9a8916",
   "metadata": {},
   "source": [
    "You can also of course try out various other operations described during the course to familiarize yourself with them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
